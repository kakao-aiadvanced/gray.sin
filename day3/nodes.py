from models import tavily_client
from graders import retrieval_grader, rag_chain, hallucination_grader, answer_grader

def web_search(state):
    """
    Web search based on the question

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Appended web results to documents
    """
    print("---WEB SEARCH---")
    question = state["question"]
    print({"question": question})

    # Web search - êµ¬ì¡°í™”ëœ ì‘ë‹µì„ ë°›ì•„ì„œ URL ì •ë³´ ë³´ì¡´
    response = tavily_client.search(
        query=question, search_depth="advanced", max_results=3
    )
    
    # Create document-like structure with source information
    from langchain_core.documents import Document
    docs = []
    
    for result in response['results']:
        doc = Document(
            page_content=result['content'],
            metadata={
                'source': result['url'],
                'title': result['title'],
                'score': result['score'],
                'source_type': 'web_search'
            }
        )
        docs.append(doc)
    
    print(f"ì›¹ ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
    for doc in docs:
        print(f"  - {doc.metadata['title']}: {doc.metadata['source']}")
    
    return {"documents": docs, "question": question}

def retrieve(state):
    """
    Retrieve documents from vectorstore

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---RETRIEVE---")
    question = state["question"]

    # Import retriever here to avoid circular imports
    from document_loader import load_existing_vectorstore
    retriever = load_existing_vectorstore()
    
    # Retrieval
    documents = retriever.invoke(question)
    
    # ë²¡í„°ìŠ¤í† ì–´ ë¬¸ì„œì— source_type ë©”íƒ€ë°ì´í„° ì¶”ê°€
    for doc in documents:
        if 'source_type' not in doc.metadata:
            doc.metadata['source_type'] = 'vector_store'
    
    print(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
    for doc in documents:
        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
        print(f"  - {title}: {source}")
    
    return {"documents": documents, "question": question}

def generate(state):
    """
    Generate answer using RAG on retrieved documents

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    print("---GENERATE---")
    question = state["question"]
    documents = state["documents"]

    # RAG generation
    generation = rag_chain.invoke({"context": documents, "question": question})
    
    # ì¶œì²˜ ì •ë³´ ì¶”ê°€
    sources = format_sources(documents)
    
    # ìƒì„±ëœ ë‹µë³€ì— ì¶œì²˜ ì •ë³´ ì¶”ê°€
    full_response = f"{generation}\n\nğŸ“š **ì¶œì²˜:**\n" + "\n".join(sources)
    
    return {"documents": documents, "question": question, "generation": full_response}

def format_sources(documents):
    """ë¬¸ì„œë“¤ì˜ ì¶œì²˜ ì •ë³´ë¥¼ í¬ë§·íŒ…"""
    sources = []
    seen_sources = set()  # ì¤‘ë³µ ì œê±°ìš©
    
    for i, doc in enumerate(documents, 1):
        source_url = doc.metadata.get('source', '')
        title = doc.metadata.get('title', f'ë¬¸ì„œ {i}')
        source_type = doc.metadata.get('source_type', 'ì•Œ ìˆ˜ ì—†ìŒ')
        
        # ì¤‘ë³µ URL ì œê±°
        if source_url and source_url not in seen_sources:
            seen_sources.add(source_url)
            if source_type == 'web_search':
                score = doc.metadata.get('score', 0)
                sources.append(f"{len(sources)+1}. **{title}** (ì›¹ ê²€ìƒ‰, ì‹ ë¢°ë„: {score:.2f})\n   ğŸ”— {source_url}")
            else:
                sources.append(f"{len(sources)+1}. **{title}** (ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤)\n   ğŸ”— {source_url}")
    
    return sources

def grade_documents(state):
    """
    Determines whether the retrieved documents are relevant to the question
    If any document is not relevant, we will set a flag to run web search

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Filtered out irrelevant documents and updated web_search state
    """
    print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
    question = state["question"]
    documents = state["documents"]

    # Score each doc
    filtered_docs = []
    web_search = "No"
    for d in documents:
        score = retrieval_grader.invoke(
            {"question": question, "document": d.page_content}
        )
        grade = score["score"]
        # Document relevant
        if grade.lower() == "yes":
            print("---GRADE: DOCUMENT RELEVANT---")
            filtered_docs.append(d)
        # Document not relevant
        else:
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            # We do not include the document in filtered_docs
            # We set a flag to indicate that we want to run web search
            web_search = "Yes"
            continue
    return {"documents": filtered_docs, "question": question, "web_search": web_search}

def route_question(state):
    """
    Route question to web search or RAG.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """
    print("---ROUTE QUESTION---")
    question = state["question"]
    print(question)
    
    from graders import question_router
    source = question_router.invoke({"question": question})
    print(source)
    print(source["datasource"])
    if source["datasource"] == "web_search":
        print("---ROUTE QUESTION TO WEB SEARCH---")
        return "websearch"
    elif source["datasource"] == "vectorstore":
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"

def decide_to_generate(state):
    """
    Determines whether to generate an answer, or add web search

    Args:
        state (dict): The current graph state

    Returns:
        str: Binary decision for next node to call
    """
    print("---ASSESS GRADED DOCUMENTS---")
    state["question"]
    web_search = state["web_search"]
    state["documents"]

    if web_search == "Yes":
        # All documents have been filtered check_relevance
        # We will re-generate a new query
        print("---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---")
        return "websearch"
    else:
        # We have relevant documents, so generate answer
        print("---DECISION: GENERATE---")
        return "generate"

def grade_generation_v_documents_and_question(state):
    """
    Determines whether the generation is grounded in the document and answers question.

    Args:
        state (dict): The current graph state

    Returns:
        str: Decision for next node to call
    """
    print("---CHECK HALLUCINATIONS---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]

    score = hallucination_grader.invoke(
        {"documents": documents, "generation": generation}
    )
    grade = score["score"]

    # Check hallucination
    if grade == "yes":
        print("---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---")
        # Check question-answering
        print("---GRADE GENERATION vs QUESTION---")
        score = answer_grader.invoke({"question": question, "generation": generation})
        grade = score["score"]
        if grade == "yes":
            print("---DECISION: GENERATION ADDRESSES QUESTION---")
            return "useful"
        else:
            print("---DECISION: GENERATION DOES NOT ADDRESS QUESTION---")
            return "not useful"
    else:
        from pprint import pprint
        pprint("---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---")
        return "not supported" 