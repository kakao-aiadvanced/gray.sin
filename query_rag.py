from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from load_blogs import get_retriever
from operator import itemgetter
from typing import List, Dict, Any, Tuple


class RAGSystemConfig:
    """RAG ì‹œìŠ¤í…œ ì„¤ì •"""
    def __init__(self):
        self.retrieval_k = 6
        self.max_attempts = 2
        self.llm_model = "gpt-4o-mini"
        self.llm_temperature = 0


class RAGSystem:
    """RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: RAGSystemConfig = None):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.config = config or RAGSystemConfig()
        
        # LLM ë° íŒŒì„œ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model=self.config.llm_model, 
            temperature=self.config.llm_temperature
        )
        self.parser = JsonOutputParser()
        
        # ì²´ì¸ë“¤ ì´ˆê¸°í™”
        self._setup_chains()
        
        # Retriever ì´ˆê¸°í™” (í•œ ë²ˆë§Œ!)
        print("ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
        self.retriever = get_retriever()
        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _setup_chains(self):
        """í”„ë¡¬í”„íŠ¸ ì²´ì¸ë“¤ ì„¤ì •"""
        # 1. ê´€ë ¨ì„± í‰ê°€ ì²´ì¸
        relevance_prompt = PromptTemplate(
            template="""You are an expert in evaluating the relevance between a user query and a retrieved document chunk.
Your task is to determine if the retrieved chunk is relevant to the user's query.
Output your answer in JSON format, using the following structure:
{{"relevance": "yes"}} if the chunk is relevant, or {{"relevance": "no"}} if it is not.

User Query: {user_query}
Retrieved Chunk: {retrieved_chunk}
{format_instructions}
""",
            input_variables=["user_query", "retrieved_chunk"],
            partial_variables={
                "format_instructions": self.parser.get_format_instructions()
            },
        )
        self.relevance_chain = relevance_prompt | self.llm | self.parser
        
        # 2. ë‹µë³€ ìƒì„± ì²´ì¸
        answer_prompt = PromptTemplate(
            template="""Answer the user query based on the provided context.
If the context does not contain enough information to answer the query, state that you cannot answer based on the provided context.
Context: {context}
User Query: {user_query}
Output your answer in JSON format, using the following structure: {{"answer": "Your answer here"}}.
{format_instructions}
""",
            input_variables=["user_query", "context"],
            partial_variables={
                "format_instructions": self.parser.get_format_instructions()
            },
        )
        self.answer_chain = answer_prompt | self.llm | self.parser
        
        # 3. Hallucination í‰ê°€ ì²´ì¸
        hallucination_prompt = PromptTemplate(
            template="""You are an expert in evaluating whether an AI-generated answer contains hallucinations.
Your task is to determine if the generated answer is faithful to the provided context and does not contain any information that is not supported by the context.

A hallucination occurs when:
1. The answer contains information that is not present in the provided context
2. The answer contradicts information in the context
3. The answer makes up facts, numbers, or details not found in the context
4. The answer draws conclusions that go beyond what the context supports

Evaluate the generated answer against the provided context and determine if there are any hallucinations.
Output your answer in JSON format, using the following structure:
{{"hallucination": "yes"}} if the answer contains hallucinations, or {{"hallucination": "no"}} if it does not.

Context: {context}
Generated Answer: {generated_answer}
{format_instructions}
""",
            input_variables=["context", "generated_answer"],
            partial_variables={
                "format_instructions": self.parser.get_format_instructions()
            },
        )
        self.hallucination_chain = hallucination_prompt | self.llm | self.parser
    
    def retrieve_documents(self, query: str) -> List[Any]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        print(f"ì‚¬ìš©ì ì¿¼ë¦¬: {query}")
        print("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        retrieved_docs = self.retriever.invoke(query)
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}")
        return retrieved_docs
    
    def evaluate_relevance(self, documents: List[Any], query: str) -> Tuple[List[str], List[Any]]:
        """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
        relevant_chunks = []
        relevant_docs = []
        
        for i, doc in enumerate(documents):
            print(f"\n--- ë¬¸ì„œ {i+1} ê´€ë ¨ì„± í‰ê°€ ---")
            chunk_content = doc.page_content
            print(f"ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {chunk_content[:100]}...")
            
            relevance_result = self.relevance_chain.invoke({
                "user_query": query, 
                "retrieved_chunk": chunk_content
            })
            print(f"ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼: {relevance_result}")
            
            if relevance_result.get('relevance') == 'yes':
                relevant_chunks.append(chunk_content)
                relevant_docs.append(doc)
                print("-> ê´€ë ¨ì„± ìˆìŒ. ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ë¨.")
            else:
                print("-> ê´€ë ¨ì„± ì—†ìŒ. ì œì™¸ë¨.")
        
        return relevant_chunks, relevant_docs
    
    def generate_answer(self, query: str, context: str) -> Dict[str, Any]:
        """ë‹µë³€ ìƒì„±"""
        return self.answer_chain.invoke({
            "user_query": query,
            "context": context
        })
    
    def check_hallucination(self, answer: str, context: str) -> Dict[str, Any]:
        """Hallucination ê²€ì‚¬"""
        return self.hallucination_chain.invoke({
            "context": context,
            "generated_answer": answer
        })
    
    def generate_answer_with_validation(self, query: str, context: str) -> Dict[str, Any]:
        """ê²€ì¦ê³¼ í•¨ê»˜ ë‹µë³€ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        attempt = 1
        
        while attempt <= self.config.max_attempts:
            print(f"\n--- ë‹µë³€ ìƒì„± (ì‹œë„ {attempt}/{self.config.max_attempts}) ---")
            answer_response = self.generate_answer(query, context)
            print(f"ìƒì„±ëœ ë‹µë³€: {answer_response}")
            
            # Hallucination í‰ê°€
            print(f"\n--- Hallucination í‰ê°€ (ì‹œë„ {attempt}) ---")
            hallucination_result = self.check_hallucination(
                answer_response.get('answer', ''), 
                context
            )
            print(f"Hallucination í‰ê°€ ê²°ê³¼: {hallucination_result}")
            
            if hallucination_result.get('hallucination') == 'no':
                print("âœ… Hallucinationì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                break
            else:
                print("âš ï¸  ê²½ê³ : ìƒì„±ëœ ë‹µë³€ì— Hallucinationì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                if attempt < self.config.max_attempts:
                    print("ğŸ”„ ë‹µë³€ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
                    attempt += 1
                else:
                    print("ğŸ“ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬. í˜„ì¬ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                    break
        
        return answer_response
    
    def format_sources(self, relevant_docs: List[Any]) -> List[str]:
        """ì¶œì²˜ ì •ë³´ í¬ë§·íŒ…"""
        sources = []
        for i, doc in enumerate(relevant_docs):
            source_info = f"ì¶œì²˜ {i+1}: "
            if hasattr(doc, 'metadata') and doc.metadata:
                if 'source' in doc.metadata:
                    source_info += f"{doc.metadata['source']}"
                if 'title' in doc.metadata:
                    source_info += f" - {doc.metadata['title']}"
            else:
                source_info += f"ë¬¸ì„œ ë‚´ìš©: {doc.page_content[:100]}..."
            sources.append(source_info)
        return sources
    
    def query(self, user_query: str) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ë©”ì¸ ì¿¼ë¦¬ ë©”ì„œë“œ"""
        print("\n" + "="*80)
        
        # 1. ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = self.retrieve_documents(user_query)
        
        # 2. ê´€ë ¨ì„± í‰ê°€
        relevant_chunks, relevant_docs = self.evaluate_relevance(retrieved_docs, user_query)
        
        if not relevant_chunks:
            print("\nê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"answer": "ì œê³µëœ ë¬¸ì„œë“¤ì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        # 3. ë‹µë³€ ìƒì„±
        print(f"\n--- ë‹µë³€ ìƒì„± ---")
        print(f"ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œ ê°œìˆ˜: {len(relevant_chunks)}")
        
        combined_context = "\n\n".join(relevant_chunks)
        answer_response = self.generate_answer_with_validation(user_query, combined_context)
        
        # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print(f"\n--- ìµœì¢… ë‹µë³€ ë° ì¶œì²˜ ---")
        print(f"ë‹µë³€: {answer_response.get('answer', '')}")
        
        # ì¶œì²˜ ì •ë³´ í‘œì‹œ
        print(f"\nğŸ“š ì¶œì²˜ ì •ë³´:")
        sources = self.format_sources(relevant_docs)
        for source in sources:
            print(source)
        
        print("="*80)
        return answer_response


# ìƒ˜í”Œ ì‹¤í–‰
if __name__ == "__main__":
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = RAGSystem()
    
    # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì¿¼ë¦¬ë“¤
    sample_queries = [
        "What is prompt engineering?",
        "What is the capital of France?"
    ]
    
    for query in sample_queries:
        result = rag_system.query(query)
        print()
